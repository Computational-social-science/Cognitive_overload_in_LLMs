{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import json\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 900\n",
    "# 将X/Y轴的刻度线方向设置向内\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "# 设置字体\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = 'Microsoft Yahei'\n",
    "# 设置公式字体\n",
    "config = {\n",
    "    \"mathtext.fontset\": 'stix',\n",
    "}\n",
    "rcParams.update(config)\n",
    "\n",
    "# 通过正则表达式匹配替换内容和语句\n",
    "def get_match_replaced(text):\n",
    "    flag = True\n",
    "    replaced_content, replaced_sentence = '', ''\n",
    "    # v2.0 prompt\n",
    "    pattern = r'A: Replaced content is <(.*?)>, Replaced sentence is <(.*?)>'\n",
    "    # 2.1 prompt\n",
    "    # pattern = r'A: Replaced content is <<<(.*?)>>>, Replaced sentence is <<<(.*?)>>>'\n",
    "    # flags=re.DOTALL 可以匹配任意字符，不会因为换行符造成错误\n",
    "    matches = list(re.finditer(pattern, text, flags=re.DOTALL))\n",
    "    if len(list(matches)) == 1:\n",
    "        for match in matches:\n",
    "            replaced_content = match.group(1)\n",
    "            replaced_sentence = match.group(2)\n",
    "    else:\n",
    "        flag = False\n",
    "    return flag, (replaced_content, replaced_sentence)\n",
    "\n",
    "\n",
    "def get_similarity_scores_BGEM2(s1, s2, model):\n",
    "    embedding_1 = model.encode([s1])['dense_vecs']\n",
    "    embedding_2 = model.encode([s2])['dense_vecs']\n",
    "    # score = embedding_1 @ embedding_2.T\n",
    "    score = util.cos_sim(embedding_1, embedding_2).item()\n",
    "    return score\n",
    "\n",
    "def get_similarity_scores_AllMpnetBaseV2(s1, s2, model):\n",
    "    embedding_1 = model.encode(s1, convert_to_tensor=True)\n",
    "    embedding_2 = model.encode(s2, convert_to_tensor=True)\n",
    "    score = util.cos_sim(embedding_1, embedding_2).item()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bgem3 = BGEM3FlagModel('data/llm/sbert/model/bge-m3', use_fp16=True)\n",
    "model_allmpnet = SentenceTransformer('./data/llm/sbert/model/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"In a word, Photography deals with what the philosopher Descartes called ‘extension’. ‘Extension’ is an attribute of things (substances) made of matter: in the philosophers own words, res extensa. Primarily, Photography, as a visual art, explores the physical side of the World, the surface of what can be caught by our eyes.\"\n",
    "s2 = \"In a word, Photography deals with what the philosopher Descartes called ‘extension’. ‘Extension’ is an attribute of things (substances) made of matter: in the philosopher's own words, res extensa. Primarily, Photography, as a visual art, explores the physical side of the World, the surface of what can be caught by our eyes.\"\n",
    "\n",
    "s1 = 'I love you'\n",
    "s2 = 'I like you'\n",
    "\n",
    "print(get_similarity_scores_AllMpnetBaseV2(s1, s2, model_allmpnet))\n",
    "print(get_similarity_scores_BGEM2(s1, s2, model_bgem3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model_name = 'llama2'\n",
    "# filepath = 'data/llm/'+ model_name +'-output/LE-dataset v2.0-150-output.json'\n",
    "filepath = 'data/llm2.1.2/'+ model_name +'-output/LE-dataset v2.1.2-150.json'\n",
    "\n",
    "print(filepath)\n",
    "llmed_list = []\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    llmed_list = json.load(file)\n",
    "print(len(llmed_list))\n",
    "# 使用tqdm创建进度条，并设置总任务数\n",
    "progress_bar = tqdm(total=len(llmed_list), desc='Progress', unit='task')\n",
    "\n",
    "# 保存包含相似度的结果\n",
    "results = []\n",
    "N, n, m = 0, 0, 0\n",
    "for i, llmed_ in enumerate(llmed_list):\n",
    "    # if i >= 10000:\n",
    "    #     break\n",
    "    # 进度条加 1\n",
    "    progress_bar.update(1)\n",
    "    \n",
    "    phrase_ = llmed_['Latin phrase']\n",
    "    local_sentence = llmed_['local sentence']\n",
    "    llmed_response = llmed_['llmed response']\n",
    "        \n",
    "     \n",
    "    if llmed_response[:8] == 'Replaced':\n",
    "        llmed_response = 'A: ' + llmed_response\n",
    "        # print(llmed_response[:11])\n",
    "    \n",
    "    if llmed_response[:11] == 'A: Replaced':\n",
    "        N += 1\n",
    "    elif llmed_response[:8] == 'Replaced':\n",
    "        n += 1\n",
    "    else:\n",
    "        m += 1    \n",
    "        \n",
    "    \n",
    "    # 比较两个句子是否符合要求\n",
    "    flag, (llmed_content, llmed_sentence) = get_match_replaced(llmed_response)\n",
    "    \n",
    "    # if not flag:\n",
    "    #     print(i, '----------------------------')\n",
    "    #     print(local_sentence)\n",
    "    #     print(llmed_response)\n",
    "    #     print(llmed_content, llmed_sentence)\n",
    "    if flag:\n",
    "        # 编码语句\n",
    "        score = get_similarity_scores_BGEM2(local_sentence, llmed_sentence, model_bgem3)\n",
    "        results.append({'Latin phrase': phrase_, 'local sentence': local_sentence, 'llmed response':llmed_sentence, 'similarity score': score})\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(N, n, m, n+N+m)\n",
    "# # 关闭进度条\n",
    "progress_bar.close()    \n",
    "\n",
    "print(len(results))\n",
    "# 保存相似度数据\n",
    "# v1.1 是指修改了匹配方法后的数据\n",
    "# out_file_path = 'data/llm/sbert/similarity-out/bge-m3/LE-dataset v2.0-150-'+ model_name +'-output(similarity) v1.1.json'\n",
    "out_file_path = 'data/llm2.1.2/sbert/similarity-out/bge-m3/LE-dataset v2.0-150-'+ model_name +'-output(similarity) v2.1.2.json'\n",
    "\n",
    "with open(out_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(results, file, indent=2)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合并图例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 计算相似度\n",
    "def get_similarity_scores(model_name):\n",
    "    # filepath = './data/llm/'+ model_name +'-output/LE-dataset v2.0-150-output.json'\n",
    "    filepath = 'data/llm/sbert/similarity-out/bge-m3/LE-dataset v2.0-150-'+ model_name +'-output(similarity) v1.1.json'\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        llmed_list = json.load(file)\n",
    "    file.close()\n",
    "    # 计算每两条语句的相似度\n",
    "    similarity_scores = []\n",
    "    for i, llmed_ in enumerate(llmed_list):\n",
    "        score = llmed_['similarity score']\n",
    "        similarity_scores.append(score)\n",
    "    return similarity_scores\n",
    "\n",
    "\n",
    "def plot_radar_map(ax, model, similarity_scores, N):\n",
    "    print('Plot radar map ' + model +'...')\n",
    "    \n",
    "    accuracy = sum(1 for ele in similarity_scores if 0.999>ele>=0.9) / len(similarity_scores)\n",
    "    ax.text(-1.55, 0.55, 'ACC ('+ model.capitalize() +') = ' + \"{:.1f}%\".format(accuracy * 100), ha='center', va='center', color='black', fontsize=14)\n",
    "    print(accuracy)\n",
    "    # 相较于 1 的偏离程度（平均绝对偏差）\n",
    "    mean_abs_deviation = np.mean(np.abs(np.array(similarity_scores) - 1))\n",
    "    print(mean_abs_deviation)\n",
    "    ax.text(-1.55, 0.48, 'MAD = ' + \"{:.3f}\".format(mean_abs_deviation), ha='center', va='center', color='black', fontsize=14)\n",
    "    \n",
    "    # 随机抽取样本量用于展示\n",
    "    similarity_scores = random.sample(similarity_scores, N)\n",
    "    categories = list(range(0, len(similarity_scores)))\n",
    "    values = similarity_scores\n",
    "\n",
    "    # 角度\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # 闭合图形\n",
    "    # 极坐标下的点位置\n",
    "    theta = angles\n",
    "    r = values + values[:1]  # 添加额外的值，使长度匹配\n",
    "    colors = ['#6671b5', '#f79779']\n",
    "    # ax.scatter(theta, r, s=50, color=colors[1], alpha=0.6)\n",
    "    ax.scatter(theta, r, marker='x', s=15, linewidth=1, color=colors[1], alpha=0.9)  \n",
    "\n",
    "    # 设置角度刻度\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    # ax.set_ylim(np.min(values), np.max(values))\n",
    "    ax.set_ylim(0.6, 1)\n",
    "    # 反转 y 轴数值\n",
    "    ax.invert_yaxis()\n",
    "    # 设置圆圈的刻度\n",
    "    ax.set_yticks([0.6, 0.7, 0.8, 0.9])\n",
    "    # 设置刻度数值的大小和颜色\n",
    "    ax.tick_params(axis='y', labelsize=12, colors='black')\n",
    "    # 控制圆圈的粗细和颜色\n",
    "    ax.grid(True, color=colors[0], linewidth=1, alpha=0.6)\n",
    "    # 不显示最外面圆圈的颜色\n",
    "    # ax.spines['polar'].set_visible(False)\n",
    "    ax.spines['polar'].set_linewidth(1.5)  # 设置最外圈的粗细为 2.0\n",
    "    ax.spines['polar'].set_color(colors[0])  # 设置最外圈的颜色为蓝色"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算Score gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statistics\n",
    "from matplotlib.patches import FancyArrow\n",
    "import json\n",
    "import textstat\n",
    "import re\n",
    "\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 900\n",
    "\n",
    "# 将X/Y轴的刻度线方向设置向内\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "# 设置字体\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = 'Microsoft Yahei'\n",
    "\n",
    "# 设置公式字体\n",
    "config = {\n",
    "    \"mathtext.fontset\": 'stix',\n",
    "}\n",
    "rcParams.update(config)\n",
    "\n",
    "    \n",
    "def appear_times_for_single(data, phrase):\n",
    "    times = 0\n",
    "    for i,ele in enumerate(data):\n",
    "        if phrase in list(ele['phrase'].keys()):\n",
    "            times += ele['phrase'][phrase]\n",
    "    return times    \n",
    "\n",
    "\n",
    "def count_bins(number):\n",
    "    n_string = '{:.15f}'.format(number)\n",
    "    decimal_part = str(n_string).split('.')[1]  # 将小数部分转换为字符串并分割\n",
    "    count = 0\n",
    "    for digit in decimal_part:\n",
    "        if digit == '0':\n",
    "            count += 1\n",
    "        else:\n",
    "            break  # 遇到第一个非零数字就停止计数\n",
    "    return count + 1\n",
    "\n",
    "\n",
    "# Michel 2007 计算频区间的中心值\n",
    "def get_center_value(freqs):\n",
    "    maxi = np.log10(max(freqs))\n",
    "    minu = np.log10(min(freqs))\n",
    "    center = (maxi + minu) / 2  # logarithmic mean 对数均值\n",
    "    return 10**center\n",
    "\n",
    "\n",
    "# 计算置信区间\n",
    "def get_confidence_interval(data, confidence_level):\n",
    "    n = len(data)  # 样本大小\n",
    "    mean = np.mean(data)  # 样本均值\n",
    "    std_err = stats.sem(data)  # 样本标准误差\n",
    "    # 使用 t 分布计算置信区间（适用于样本较小的情况）\n",
    "    t_value = stats.t.ppf((1 + confidence_level) / 2, df=n-1)  # t 分布临界值\n",
    "    margin_of_error = t_value * std_err  # 误差范围\n",
    "    confidence_interval = (mean - margin_of_error, mean + margin_of_error)  # 置信区间\n",
    "    return confidence_interval\n",
    "\n",
    "\n",
    "def search(data, phrase):\n",
    "    results = []\n",
    "    for ele in data:\n",
    "        if phrase == ele['Latin phrase']:\n",
    "            results.append(ele)\n",
    "    return results\n",
    "            \n",
    "            \n",
    "# 获取拉丁文本的音节数量\n",
    "def get_latin_syllable(latin_):\n",
    "    # 拉丁单词的音节数量为 元音+双元音 的数量\n",
    "    pattern = re.compile('(ae|au|ei|eu|oe|ui|[aeiouy])')\n",
    "    count = len(pattern.findall(latin_))\n",
    "    return count\n",
    "\n",
    "\n",
    "# 获取 Flesch kincaid grade\n",
    "def get_flesch_kincaid_grade(latin_phrase, mixed_, llmed_):\n",
    "    latin_syllable = get_latin_syllable(latin_phrase)\n",
    "    total_syllable = textstat.syllable_count(mixed_.replace(latin_phrase, '')) + latin_syllable\n",
    "    total_words = textstat.lexicon_count(mixed_, removepunct=True)\n",
    "    total_sentences = textstat.sentence_count(mixed_)\n",
    "    mixed_score = 0.39 * (total_words / total_sentences) + 11.8 * (total_syllable / total_words) - 15.59\n",
    "    # print(total_sentences, total_words, total_syllable)\n",
    "    llmed_score = textstat.flesch_kincaid_grade(llmed_)\n",
    "    rate = (mixed_score - llmed_score) / (((mixed_score + llmed_score)) / 2)\n",
    "    \n",
    "    return mixed_score, llmed_score, rate\n",
    "\n",
    "\n",
    "def get_flesch_reading_ease(latin_phrase, mixed_, llmed_):\n",
    "    # 计算ASL（平均句子长度）\n",
    "    sentence_count = textstat.sentence_count(mixed_)  # 获取句子数\n",
    "    word_count = textstat.lexicon_count(mixed_)  # 获取单词数\n",
    "    asl = word_count / sentence_count\n",
    "    # 计算ASW（平均每个单词的音节数）\n",
    "    latin_syllable = get_latin_syllable(latin_phrase)\n",
    "    total_syllable = textstat.syllable_count(mixed_.replace(latin_phrase, '')) + latin_syllable\n",
    "    asw = total_syllable / word_count\n",
    "    mixed_score = 206.835 - (1.015 * asl) - (84.6 * asw)\n",
    "    llmed_score = textstat.flesch_reading_ease(llmed_)\n",
    "    rate = (mixed_score - llmed_score) / (((mixed_score + llmed_score)) / 2)\n",
    "    \n",
    "    return mixed_score, llmed_score, rate\n",
    "\n",
    "\n",
    "def get_smog_index(latin_phrase, mixed_, llmed_):\n",
    "    # 多音节单词数量\n",
    "    polysyllable_count = 0\n",
    "    for word in latin_phrase.split():\n",
    "        if get_latin_syllable(word) >= 3:\n",
    "            polysyllable_count += 1\n",
    "    for word in mixed_.replace(latin_phrase, '').split():\n",
    "        syllable_count = textstat.syllable_count(word)\n",
    "        if syllable_count >= 3:\n",
    "            polysyllable_count += 1\n",
    "    mixed_score = 1.043 * np.sqrt(polysyllable_count * (30 / textstat.sentence_count(mixed_))) + 3.1291\n",
    "    llmed_score = textstat.smog_index(llmed_)\n",
    "    rate = (mixed_score - llmed_score) / (((mixed_score + llmed_score)) / 2)\n",
    "    \n",
    "    return mixed_score, llmed_score, rate\n",
    "\n",
    "\n",
    "def get_coleman_liau_index(latin_phrase, mixed_, llmed_):\n",
    "    mixed_score = textstat.coleman_liau_index(mixed_)\n",
    "    llmed_score = textstat.coleman_liau_index(llmed_)\n",
    "    if mixed_score == 0 and llmed_score == 0:\n",
    "        return 0, 0, 0\n",
    "    else:\n",
    "        rate = (mixed_score - llmed_score) / (((mixed_score + llmed_score)) / 2)\n",
    "        \n",
    "        return mixed_score, llmed_score, rate\n",
    "\n",
    "\n",
    "def get_gunning_fog_index(latin_phrase, mixed_, llmed_):\n",
    "    # 多音节单词数量\n",
    "    polysyllable_count = 0\n",
    "    for word in latin_phrase.split():\n",
    "        if get_latin_syllable(word) >= 3:\n",
    "            polysyllable_count += 1\n",
    "    for word in mixed_.replace(latin_phrase, '').split():\n",
    "        syllable_count = textstat.syllable_count(word)\n",
    "        if syllable_count >= 3:\n",
    "            polysyllable_count += 1\n",
    "    total_words = textstat.lexicon_count(mixed_, removepunct=True)\n",
    "    total_sentences = textstat.sentence_count(mixed_)\n",
    "    complex_words = polysyllable_count\n",
    "    mixed_score = 0.4 * ( (total_words / total_sentences) + 100 * (complex_words / total_words) )\n",
    "    llmed_score = textstat.gunning_fog(llmed_)\n",
    "    rate = (mixed_score - llmed_score) / (((mixed_score + llmed_score)) / 2)\n",
    "    \n",
    "    return mixed_score, llmed_score, rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_llmed(model):\n",
    "    filepath = 'data/llm2.1.2/sbert/similarity-out/bge-m3/LE-dataset v2.0-150-'+ model +'-output(similarity) v2.1.2.json'\n",
    "    llmed_list = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        llmed_list = json.load(file)\n",
    "    file.close()\n",
    "    return llmed_list\n",
    "\n",
    "\n",
    "def get_score_dict(llmed_list, calculate_index):\n",
    "    j = 0\n",
    "    n = 0\n",
    "    score_dict = {}\n",
    "    for i, ele in enumerate(llmed_list): \n",
    "        # <<<<<<使用带有相似度数据集>>>>>>\n",
    "        latin_phrase = ele['Latin phrase']\n",
    "        local_sentence = ele['local sentence']\n",
    "        llmed_response = ele['llmed response']\n",
    "        score = ele['similarity score']\n",
    "        \n",
    "        llmed_sentence = ''\n",
    "        # if 0.9999999 > score >= 0.9:\n",
    "        if 0.999 > score >= 0.9:\n",
    "            llmed_sentence = llmed_response\n",
    "            n += 1\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # 选择评价指标\n",
    "        s1, s2, rate = 0, 0, 0\n",
    "        if calculate_index == 'FKGL':\n",
    "            s1, s2, rate = get_flesch_kincaid_grade(latin_phrase, local_sentence, llmed_sentence)\n",
    "        elif calculate_index == 'FRE':\n",
    "            s1, s2, rate = get_flesch_reading_ease(latin_phrase, local_sentence, llmed_sentence)\n",
    "        elif calculate_index == 'GFI':\n",
    "            s1, s2, rate = get_gunning_fog_index(latin_phrase, local_sentence, llmed_sentence)\n",
    "        elif calculate_index == 'SMOG':\n",
    "            s1, s2, rate = get_smog_index(latin_phrase, local_sentence, llmed_sentence)\n",
    "        elif calculate_index == 'CLI':\n",
    "            s1, s2, rate = get_coleman_liau_index(latin_phrase, local_sentence, llmed_sentence)\n",
    "\n",
    "        if s1 - s2 > 0:\n",
    "            j += 1\n",
    "        # 构建 {'de facto': [0.23, 0.25]} 词组-分差 对\n",
    "        para = s1 - s2\n",
    "        # para = rate\n",
    "        if latin_phrase not in list(score_dict.keys()):\n",
    "            score_dict[latin_phrase] = [para]\n",
    "        else :\n",
    "            score_dict[latin_phrase].append(para)\n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rate_all = {}\n",
    "\n",
    "model = 'llama2'\n",
    "# FKGL FRE SMOG CL\n",
    "indexs = ['FKGL', 'FRE', 'GFI', 'SMOG', 'CLI']\n",
    "for index in indexs:\n",
    "    dict_temp = get_score_dict(read_llmed(model), index)\n",
    "    dict_rate_v = {}\n",
    "    for key, val in dict_temp.items():\n",
    "        rate = np.median(val)\n",
    "        # rate = np.average(val)\n",
    "        dict_rate_v[key] = rate\n",
    "    dict_rate_all[index] = dict_rate_v\n",
    "    print(dict_rate_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_path = './data./llm2.1.2/sbert/score-gap/LE-dataset v2.0-150-'+ model +'-score-gap v2.1.2.json'\n",
    "with open(out_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(dict_rate_all, file, indent=2)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SBERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
