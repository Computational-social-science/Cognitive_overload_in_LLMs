{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leisaihua/miniconda3/envs/swift/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/leisaihua/miniconda3/envs/swift/lib/python3.10/site-packages/awq/modules/linear/exllama.py:12: UserWarning: AutoAWQ could not load ExLlama kernels extension. Details: /home/leisaihua/miniconda3/envs/swift/lib/python3.10/site-packages/exl_ext.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi\n",
      "  warnings.warn(f\"AutoAWQ could not load ExLlama kernels extension. Details: {ex}\")\n",
      "/home/leisaihua/miniconda3/envs/swift/lib/python3.10/site-packages/awq/modules/linear/exllamav2.py:13: UserWarning: AutoAWQ could not load ExLlamaV2 kernels extension. Details: /home/leisaihua/miniconda3/envs/swift/lib/python3.10/site-packages/exlv2_ext.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi\n",
      "  warnings.warn(f\"AutoAWQ could not load ExLlamaV2 kernels extension. Details: {ex}\")\n",
      "/home/leisaihua/miniconda3/envs/swift/lib/python3.10/site-packages/awq/modules/linear/gemm.py:14: UserWarning: AutoAWQ could not load GEMM kernels extension. Details: /home/leisaihua/miniconda3/envs/swift/lib/python3.10/site-packages/awq_ext.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi\n",
      "  warnings.warn(f\"AutoAWQ could not load GEMM kernels extension. Details: {ex}\")\n",
      "/home/leisaihua/miniconda3/envs/swift/lib/python3.10/site-packages/awq/modules/linear/gemv.py:11: UserWarning: AutoAWQ could not load GEMV kernels extension. Details: /home/leisaihua/miniconda3/envs/swift/lib/python3.10/site-packages/awq_ext.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi\n",
      "  warnings.warn(f\"AutoAWQ could not load GEMV kernels extension. Details: {ex}\")\n",
      "[INFO:swift] Successfully registered `/home/leisaihua/workspace/LLM/swift/swift/llm/data/dataset_info.json`\n",
      "2024-08-14 20:04:12,946\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "[INFO:swift] Loading the model using model_dir: /data/leisaihua/models/SOLAR-10.7B-Instruct-v1.0\n",
      "[INFO:swift] model_config: LlamaConfig {\n",
      "  \"_name_or_path\": \"/data/leisaihua/models/SOLAR-10.7B-Instruct-v1.0\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.44.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 20:04:15 config.py:729] Defaulting to use mp for distributed inference\n",
      "INFO 08-14 20:04:16 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/data/leisaihua/models/SOLAR-10.7B-Instruct-v1.0', speculative_config=None, tokenizer='/data/leisaihua/models/SOLAR-10.7B-Instruct-v1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=42, served_model_name=/data/leisaihua/models/SOLAR-10.7B-Instruct-v1.0, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 08-14 20:04:16 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 08-14 20:04:16 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2837378)\u001b[0;0m INFO 08-14 20:04:20 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 08-14 20:04:20 utils.py:841] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2837378)\u001b[0;0m INFO 08-14 20:04:20 utils.py:841] Found nccl from library libnccl.so.2\n",
      "INFO 08-14 20:04:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2837378)\u001b[0;0m INFO 08-14 20:04:20 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 08-14 20:04:21 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f4c3ea99690>, local_subscribe_port=52633, remote_subscribe_port=None)\n",
      "INFO 08-14 20:04:21 model_runner.py:720] Starting to load model /data/leisaihua/models/SOLAR-10.7B-Instruct-v1.0...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2837378)\u001b[0;0m INFO 08-14 20:04:21 model_runner.py:720] Starting to load model /data/leisaihua/models/SOLAR-10.7B-Instruct-v1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:16<01:06, 16.71s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:37<00:58, 19.38s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:55<00:36, 18.42s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [01:03<00:14, 14.42s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:34<00:00, 20.27s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [01:34<00:00, 18.84s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 20:05:55 model_runner.py:732] Loading model weights took 9.9978 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2837378)\u001b[0;0m INFO 08-14 20:05:55 model_runner.py:732] Loading model weights took 9.9978 GB\n",
      "INFO 08-14 20:05:57 distributed_gpu_executor.py:56] # GPU blocks: 7281, # CPU blocks: 2730\n",
      "INFO 08-14 20:05:59 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-14 20:05:59 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2837378)\u001b[0;0m INFO 08-14 20:05:59 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2837378)\u001b[0;0m INFO 08-14 20:05:59 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2837378)\u001b[0;0m INFO 08-14 20:06:04 model_runner.py:1225] Graph capturing finished in 5 secs.\n",
      "INFO 08-14 20:06:05 model_runner.py:1225] Graph capturing finished in 6 secs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "dirs = [\"/home/leisaihua/workspace/LLM/swift/examples/pytorch/llm\"]\n",
    "for _dir in dirs:\n",
    "    if _dir not in sys.path:\n",
    "        sys.path.append(_dir)\n",
    "\n",
    "from swift.llm import (\n",
    "    ModelType, get_vllm_engine, get_default_template_type,\n",
    "    get_template, inference_vllm, VllmGenerationConfig\n",
    ")\n",
    "from custom import CustomModelType, CustomTemplateType\n",
    "\n",
    "# 官方方法选择模型\n",
    "# model_type = ModelType.mistral_7b_instruct_v3\n",
    "# model_type = ModelType.llama3_70b_instruct_awq\n",
    "# 从custom.py中设定模型\n",
    "# model_type = ModelType.llama3_8b_instruct\n",
    "# model_type = ModelType.llama3_1_8b_instruct\n",
    "# model_type = CustomModelType.mixtral_moe_7b_instruct_awq\n",
    "model_type = CustomModelType.solar_instruct_10_7b\n",
    "\n",
    "\n",
    "llm_engine = get_vllm_engine(\n",
    "    model_type, \n",
    "    # 使用官方方法，需要指定模型路径，否则会自己下载模型\n",
    "    model_id_or_path=\"/data/leisaihua/models/SOLAR-10.7B-Instruct-v1.0\",\n",
    "    # model_id_or_path=\"/home/css/models/Mistral-7B-Instruct-v0.3\",\n",
    "    tensor_parallel_size=2,\n",
    "    max_model_len=4096,\n",
    "    engine_kwargs = {\n",
    "        # \"enforce_eager\": True,\n",
    "        \"max_num_seqs\": 64,\n",
    "        \"seed\": 42,\n",
    "    }\n",
    ")\n",
    "\n",
    "template_type = get_default_template_type(model_type)\n",
    "template = get_template(template_type, llm_engine.hf_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64847 {'query': \"Replace the Latin phrase in the sentence with equivalent English content without changing the original meaning. Just tell me the answer, no explanation. You should respond in the following format: A: Replaced content is <###>, Replaced sentence is <###>.\\nExamples\\nQ: Latin phrase is <de facto>, Sentence is <Previously, Disney had de facto control of the board as the district's largest landowner.>\\nA: Replaced content is <actual>, Replaced sentence is <Previously, Disney had actually control of the board as the district's largest landowner.>\\nQ: Latin phrase is <sub rosa>, Sentence is <The idea that that energy is continuing 25 years later, in a kind of sub rosa away, was very fun for us.>\\nA: Replaced content is <secret>, Replaced sentence is <The idea that that energy is continuing 25 years later, in a kind of secret way, was very fun for us.>\\nQuestion:\\nQ: Latin phrase is <pros and cons>, Sentence is <Investing in commercial real estate comes with pros and cons.>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "from utils import tools\n",
    "import json\n",
    "\n",
    "\n",
    "filepath = 'utils/data/LE-dataset v2.1/llama3-newdata/LE-dataset v2.1.2-150.json'\n",
    "\n",
    "request_list = []\n",
    "latin_phrase = []\n",
    "local_sentence = []\n",
    "with open(filepath, 'r') as file:\n",
    "    contents = json.load(file)\n",
    "    for i, ele in enumerate(contents):\n",
    "        # if i > 100:\n",
    "        #     break\n",
    "        phrase = ele['Latin phrase']\n",
    "        text = ele['llmed sentence']\n",
    "        eval_prompt = f\"Replace the Latin phrase in the sentence with equivalent English content without changing the original meaning. Just tell me the answer, no explanation. You should respond in the following format: A: Replaced content is <###>, Replaced sentence is <###>.\\nExamples\\nQ: Latin phrase is <de facto>, Sentence is <Previously, Disney had de facto control of the board as the district's largest landowner.>\\nA: Replaced content is <actual>, Replaced sentence is <Previously, Disney had actually control of the board as the district's largest landowner.>\\nQ: Latin phrase is <sub rosa>, Sentence is <The idea that that energy is continuing 25 years later, in a kind of sub rosa away, was very fun for us.>\\nA: Replaced content is <secret>, Replaced sentence is <The idea that that energy is continuing 25 years later, in a kind of secret way, was very fun for us.>\\nQuestion:\\nQ: Latin phrase is <{phrase}>, Sentence is <{text}>\\n\"\n",
    "        # 获取模型的回答\n",
    "        request_list.append({'query': eval_prompt})\n",
    "        latin_phrase.append(phrase)\n",
    "        local_sentence.append(text)\n",
    "print(len(request_list), request_list[0])\n",
    "\n",
    "\n",
    "# content_list = []\n",
    "\n",
    "# print(\"需要生成的语句数量：\", len(content_list))\n",
    "# # 请求内容\n",
    "# request_list = []\n",
    "# latin_phrase = []\n",
    "# local_sentence = []\n",
    "# for i in range(0, len(content_list)):\n",
    "#     content_tuple = content_list[i]\n",
    "#     phrase, text = content_tuple\n",
    "#     phrase = str(phrase)\n",
    "#     text = str(text)\n",
    "#     # 设计提示词\n",
    "#     eval_prompt = f\"Replace the Latin phrase in the sentence with equivalent English content without changing the original meaning. Just tell me the answer, no explanation. You should respond in the following format: A: Replaced content is <###>, Replaced sentence is <###>.\\nExamples\\nQ: Latin phrase is <de facto>, Sentence is <Previously, Disney had de facto control of the board as the district's largest landowner.>\\nA: Replaced content is <actual>, Replaced sentence is <Previously, Disney had actually control of the board as the district's largest landowner.>\\nQ: Latin phrase is <sub rosa>, Sentence is <The idea that that energy is continuing 25 years later, in a kind of sub rosa away, was very fun for us.>\\nA: Replaced content is <secret>, Replaced sentence is <The idea that that energy is continuing 25 years later, in a kind of secret way, was very fun for us.>\\nQuestion:\\nQ: Latin phrase is <{phrase}>, Sentence is <{text}>\\n\"\n",
    "    \n",
    "#     # eval_prompt = f\"Replace the Latin phrase in the sentence with equivalent English content without changing the original meaning. Just tell me the answer, no explanation. You should respond in the following format: A: Replaced content is <<<###>>>, Replaced sentence is <<<###>>>.\\nExamples\\nQ: Latin phrase is <<<de facto>>>, Sentence is <<<Previously, Disney had de facto control of the board as the district's largest landowner.>>>\\nA: Replaced content is <<<actual>>>, Replaced sentence is <<<Previously, Disney had actually control of the board as the district's largest landowner.>>>\\nQ: Latin phrase is <<<sub rosa>>>, Sentence is <<<The idea that that energy is continuing 25 years later, in a kind of sub rosa away, was very fun for us.>>>\\nA: Replaced content is <<<secret>>>, Replaced sentence is <<<The idea that that energy is continuing 25 years later, in a kind of secret way, was very fun for us.>>>\\nQuestion:\\nQ: Latin phrase is <<<{phrase}>>>, Sentence is <<<{text}>>>\\n\"\n",
    "        \n",
    "#     # 获取模型的回答\n",
    "#     request_list.append({'query': eval_prompt})\n",
    "#     latin_phrase.append(phrase)\n",
    "#     local_sentence.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64847/64847 [1:19:04<00:00, 13.67it/s]\n"
     ]
    }
   ],
   "source": [
    "generation_config = VllmGenerationConfig(\n",
    "    max_new_tokens=4096,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 请求内容格式\n",
    "# request_list = [\n",
    "#     {'query': \"Hi?\"},\n",
    "# ]\n",
    "\n",
    "# 回答内容\n",
    "resp_list = inference_vllm(\n",
    "    llm_engine, template, request_list, generation_config=generation_config, \n",
    "    # 显示进度条\n",
    "    use_tqdm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64847\n"
     ]
    }
   ],
   "source": [
    "# 保存内容\n",
    "response_list = []\n",
    "for i, (phrase, text, request, resp) in enumerate(zip(latin_phrase, local_sentence, request_list, resp_list)):\n",
    "    # print(i+1, '------------------------------------')\n",
    "    # print(f\"query:\\n{request['query']}\")\n",
    "    # print(f\"response:\\n{resp['response']}\")\n",
    "    # print(i+1, resp['response'])\n",
    "    response = str(resp['response'])    \n",
    "    \n",
    "    # llmed response: 大模型的回答\n",
    "    response_list.append({'Latin phrase': phrase, 'local sentence': text, 'llmed response': response})\n",
    "print(len(response_list))\n",
    "# 指定本地文件路径\n",
    "out_filepath = './utils/data/LE-dataset v2.1/llm-output v2.1/solar-out/LE-dataset v2.1.2-150.json'\n",
    "tools.save_response(out_filepath, response_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用LLMs生成新的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要生成的语句数量： 66627\n"
     ]
    }
   ],
   "source": [
    "from utils import tools\n",
    "\n",
    "\n",
    "filepath = './utils/data/LE-dataset v2.1/LE-dataset v2.1-150.json'\n",
    "# 生成用于微调的数据集 892条\n",
    "content_list = tools.get_LE_data(filepath)\n",
    "print(\"需要生成的语句数量：\", len(content_list))\n",
    "\n",
    "# 请求内容\n",
    "request_list = []\n",
    "latin_phrase = []\n",
    "local_sentence = []\n",
    "for i in range(0, len(content_list)):\n",
    "    content_tuple = content_list[i]\n",
    "    phrase, text = content_tuple\n",
    "    phrase = str(phrase)\n",
    "    text = str(text)\n",
    "    # 设计提示词\n",
    "    # eval_prompt = f\"Replace the Latin phrase in the sentence with equivalent English content without changing the original meaning. Just tell me the answer, no explanation. You should respond in the following format: A: Replaced content is <###>, Replaced sentence is <###>.\\nExamples\\nQ: Latin phrase is <de facto>, Sentence is <Previously, Disney had de facto control of the board as the district's largest landowner.>\\nA: Replaced content is <actual>, Replaced sentence is <Previously, Disney had actually control of the board as the district's largest landowner.>\\nQ: Latin phrase is <sub rosa>, Sentence is <The idea that that energy is continuing 25 years later, in a kind of sub rosa away, was very fun for us.>\\nA: Replaced content is <secret>, Replaced sentence is <The idea that that energy is continuing 25 years later, in a kind of secret way, was very fun for us.>\\nQuestion:\\nQ: Latin phrase is <{phrase}>, Sentence is <{text}>\\n\"\n",
    "    eval_prompt = f\"Extract the following sentences that contain Latin phrase. Just tell the extracted sentence.\\nQ: Latin phrase is <jus ad bellu>, the contence is <'g. the jus ad bellum) are, arguably, one of the greatest obstacles with regard to achieving a lasting Asian and European peace. A few examples should amply demonstrate why.'>.\\nA: Extracted sentence is <The jus ad bellum are, arguably, one of the greatest obstacles with regard to achieving a lasting Asian and European peace.>.\\nQ: Latin phrase is <ex vivo>, the sentnece is <E. Limitations of ex vivo measurements for in vivo neuroscience. Proc.>.\\nA: Extracted sentence is <Limitations of ex vivo measurements for in vivo neuroscience.>.\\nQ: Latin phrase is <{phrase}>, the sentnece is <{text}>.\\n\"\n",
    "    \n",
    "    # eval_prompt = f\"Replace the Latin phrase in the sentence with equivalent English content without changing the original meaning. Just tell me the answer, no explanation. You should respond in the following format: A: Replaced content is <<<###>>>, Replaced sentence is <<<###>>>.\\nExamples\\nQ: Latin phrase is <<<de facto>>>, Sentence is <<<Previously, Disney had de facto control of the board as the district's largest landowner.>>>\\nA: Replaced content is <<<actual>>>, Replaced sentence is <<<Previously, Disney had actually control of the board as the district's largest landowner.>>>\\nQ: Latin phrase is <<<sub rosa>>>, Sentence is <<<The idea that that energy is continuing 25 years later, in a kind of sub rosa away, was very fun for us.>>>\\nA: Replaced content is <<<secret>>>, Replaced sentence is <<<The idea that that energy is continuing 25 years later, in a kind of secret way, was very fun for us.>>>\\nQuestion:\\nQ: Latin phrase is <<<{phrase}>>>, Sentence is <<<{text}>>>\\n\"\n",
    "        \n",
    "    # 获取模型的回答\n",
    "    request_list.append({'query': eval_prompt})\n",
    "    latin_phrase.append(phrase)\n",
    "    local_sentence.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 127/66627 [00:31<4:06:08,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:13:51 scheduler.py:1099] Sequence group 163 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1814/66627 [06:57<4:03:52,  4.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:20:17 scheduler.py:1099] Sequence group 1846 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 3160/66627 [12:37<6:10:50,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:25:57 scheduler.py:1099] Sequence group 3202 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4824/66627 [19:00<3:50:40,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:32:20 scheduler.py:1099] Sequence group 4862 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 6535/66627 [25:29<3:53:33,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:38:49 scheduler.py:1099] Sequence group 6570 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 8112/66627 [31:35<3:43:24,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:44:55 scheduler.py:1099] Sequence group 8151 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 9982/66627 [38:45<3:29:41,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:52:05 scheduler.py:1099] Sequence group 10016 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 11939/66627 [46:07<3:44:19,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 13:59:27 scheduler.py:1099] Sequence group 11975 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 13431/66627 [51:48<2:59:58,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:05:08 scheduler.py:1099] Sequence group 13466 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 15295/66627 [58:51<3:13:36,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:12:11 scheduler.py:1099] Sequence group 15333 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 16874/66627 [1:05:05<3:26:41,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:18:25 scheduler.py:1099] Sequence group 16909 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 18444/66627 [1:11:11<2:56:00,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:24:31 scheduler.py:1099] Sequence group 18483 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 20644/66627 [1:19:24<2:49:57,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:32:45 scheduler.py:1099] Sequence group 20679 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 22343/66627 [1:25:56<3:40:21,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:39:16 scheduler.py:1099] Sequence group 22377 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 24275/66627 [1:33:13<3:00:10,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:46:33 scheduler.py:1099] Sequence group 24314 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 25961/66627 [1:39:35<2:43:08,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:52:55 scheduler.py:1099] Sequence group 25993 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 27628/66627 [1:46:24<2:51:55,  3.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 14:59:45 scheduler.py:1099] Sequence group 27657 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 28844/66627 [1:51:24<3:02:58,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 15:04:44 scheduler.py:1099] Sequence group 28860 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 30653/66627 [1:58:21<2:31:51,  3.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 15:11:41 scheduler.py:1099] Sequence group 30685 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 32727/66627 [2:06:02<2:36:49,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 15:19:23 scheduler.py:1099] Sequence group 32764 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 34297/66627 [2:11:56<2:12:39,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 15:25:16 scheduler.py:1099] Sequence group 34331 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 36214/66627 [2:19:06<2:14:58,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 15:32:26 scheduler.py:1099] Sequence group 36253 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 37910/66627 [2:25:13<2:06:26,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 15:38:33 scheduler.py:1099] Sequence group 37943 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 39617/66627 [2:31:57<1:54:28,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 15:45:17 scheduler.py:1099] Sequence group 39649 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 41241/66627 [2:38:14<1:47:46,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 15:51:35 scheduler.py:1099] Sequence group 41278 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 43136/66627 [2:45:31<1:33:01,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 15:58:51 scheduler.py:1099] Sequence group 43174 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 45022/66627 [2:53:13<1:29:02,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:06:33 scheduler.py:1099] Sequence group 45056 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 46571/66627 [2:59:22<1:19:29,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:12:42 scheduler.py:1099] Sequence group 46600 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 48234/66627 [3:06:10<1:23:07,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:19:30 scheduler.py:1099] Sequence group 48267 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 49879/66627 [3:12:45<1:07:03,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:26:06 scheduler.py:1099] Sequence group 49914 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 51213/66627 [3:18:08<54:23,  4.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:31:28 scheduler.py:1099] Sequence group 51248 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 52856/66627 [3:24:50<56:16,  4.08it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:38:10 scheduler.py:1099] Sequence group 52892 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 54072/66627 [3:29:52<46:34,  4.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:43:13 scheduler.py:1099] Sequence group 54105 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 55489/66627 [3:35:49<48:30,  3.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:49:09 scheduler.py:1099] Sequence group 55526 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 57075/66627 [3:42:16<32:50,  4.85it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 16:55:36 scheduler.py:1099] Sequence group 57114 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 58886/66627 [3:49:24<27:17,  4.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 17:02:44 scheduler.py:1099] Sequence group 58923 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 60436/66627 [3:55:39<24:21,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 17:09:00 scheduler.py:1099] Sequence group 60471 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 62062/66627 [4:02:04<14:22,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 17:15:24 scheduler.py:1099] Sequence group 62098 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 63683/66627 [4:08:29<11:17,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 17:21:49 scheduler.py:1099] Sequence group 63722 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 65659/66627 [4:16:12<04:18,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-11 17:29:32 scheduler.py:1099] Sequence group 65694 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66627/66627 [4:19:59<00:00,  4.27it/s]\n"
     ]
    }
   ],
   "source": [
    "generation_config = VllmGenerationConfig(\n",
    "    max_new_tokens=4096,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 请求内容格式\n",
    "# request_list = [\n",
    "#     {'query': \"Hi?\"},\n",
    "# ]\n",
    "\n",
    "# 回答内容\n",
    "resp_list = inference_vllm(\n",
    "    llm_engine, template, request_list, generation_config=generation_config, \n",
    "    # 显示进度条\n",
    "    use_tqdm=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64847\n"
     ]
    }
   ],
   "source": [
    "# 保存内容\n",
    "response_list = []\n",
    "for i, (phrase, text, request, resp) in enumerate(zip(latin_phrase, local_sentence, request_list, resp_list)):\n",
    "    # print(i+1, '------------------------------------')\n",
    "    # print(f\"query:\\n{request['query']}\")\n",
    "    # print(f\"response:\\n{resp['response']}\")\n",
    "    # print(i+1, resp['response'])\n",
    "    response = str(resp['response'])    \n",
    "    import re\n",
    "    \n",
    "    pattern = r\"<(.*?)>\"\n",
    "    \n",
    "    matches = list(re.finditer(pattern, response, flags=re.DOTALL))\n",
    "    # print(len(matches))\n",
    "    if len(list(matches)) == 1:\n",
    "        for match in matches:\n",
    "            replaced_content = match.group(1)\n",
    "            # print(replaced_content)\n",
    "            if phrase not in replaced_content:\n",
    "                continue\n",
    "            \n",
    "            # llmed response: 大模型的回答\n",
    "            response_list.append({'Latin phrase': phrase, 'local sentence': text, 'llmed response': response, 'llmed sentence': replaced_content})\n",
    "print(len(response_list))\n",
    "# 指定本地文件路径\n",
    "out_filepath = './utils/data/LE-dataset v2.1/llama3-newdata/LE-dataset v2.1.2-150.json'\n",
    "tools.save_response(out_filepath, response_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
